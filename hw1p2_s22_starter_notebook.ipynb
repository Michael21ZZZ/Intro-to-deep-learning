{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MIby3J0IWvkY",
   "metadata": {
    "id": "MIby3J0IWvkY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "!pip install --upgrade --force-reinstall --no-deps kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Qcny8yCsWxmq",
   "metadata": {
    "id": "Qcny8yCsWxmq"
   },
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # TODO: Please try different architectures\n",
    "        in_size = 13\n",
    "        layers = [\n",
    "            nn.Linear(in_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 40)\n",
    "        ]\n",
    "        self.laysers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, A0):\n",
    "        x = self.laysers(A0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Jr9VJwbGWxrY",
   "metadata": {
    "id": "Jr9VJwbGWxrY"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LibriSamples(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, sample=20000, shuffle=True, partition=\"dev-clean\", csvpath=None):\n",
    "        # sample represent how many npy files will be preloaded for one __getitem__ call\n",
    "        self.sample = sample \n",
    "        \n",
    "        self.X_dir = data_path + \"/\" + partition + \"/mfcc/\"\n",
    "        self.Y_dir = data_path + \"/\" + partition +\"/transcript/\"\n",
    "        \n",
    "        self.X_names = os.listdir(self.X_dir)\n",
    "        self.Y_names = os.listdir(self.Y_dir)\n",
    "\n",
    "        # using a small part of the dataset to debug\n",
    "        if csvpath:\n",
    "            subset = self.parse_csv(csvpath)\n",
    "            self.X_names = [i for i in self.X_names if i in subset]\n",
    "            self.Y_names = [i for i in self.Y_names if i in subset]\n",
    "        \n",
    "        if shuffle == True:\n",
    "            XY_names = list(zip(self.X_names, self.Y_names))\n",
    "            random.shuffle(XY_names)\n",
    "            self.X_names, self.Y_names = zip(*XY_names)\n",
    "        \n",
    "        assert(len(self.X_names) == len(self.Y_names))\n",
    "        self.length = len(self.X_names)\n",
    "        \n",
    "        self.PHONEMES = [\n",
    "            'SIL',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',  \n",
    "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
    "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
    "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
    "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
    "            'V',     'W',     'Y',     'Z',     'ZH',    '<sos>', '<eos>']\n",
    "      \n",
    "    @staticmethod\n",
    "    def parse_csv(filepath):\n",
    "        subset = []\n",
    "        with open(filepath) as f:\n",
    "            f_csv = csv.reader(f)\n",
    "            for row in f_csv:\n",
    "                subset.append(row[1])\n",
    "        return subset[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.length / self.sample))\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        sample_range = range(i*self.sample, min((i+1)*self.sample, self.length))\n",
    "        \n",
    "        X, Y = [], []\n",
    "        for j in sample_range:\n",
    "            X_path = self.X_dir + self.X_names[j]\n",
    "            Y_path = self.Y_dir + self.Y_names[j]\n",
    "            \n",
    "            label = [self.PHONEMES.index(yy) for yy in np.load(Y_path)][1:-1]\n",
    "\n",
    "            X_data = np.load(X_path)\n",
    "            X_data = (X_data - X_data.mean(axis=0))/X_data.std(axis=0)\n",
    "            X.append(X_data)\n",
    "            Y.append(np.array(label))\n",
    "            \n",
    "        X, Y = np.concatenate(X), np.concatenate(Y)\n",
    "        return X, Y\n",
    "    \n",
    "class LibriItems(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y, context = 0):\n",
    "        assert(X.shape[0] == Y.shape[0])\n",
    "        \n",
    "        self.length  = X.shape[0]\n",
    "        self.context = context\n",
    "\n",
    "        if context == 0:\n",
    "            self.X, self.Y = X, Y\n",
    "        else:\n",
    "            X = np.pad(X, ((context,context), (0,0)), 'constant', constant_values=(0,0))\n",
    "            self.X, self.Y = X, Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        if self.context == 0:\n",
    "            xx = self.X[i].flatten()\n",
    "            yy = self.Y[i]\n",
    "        else:\n",
    "            xx = self.X[i:(i + 2*self.context + 1)].flatten()\n",
    "            yy = self.Y[i]\n",
    "        return xx, yy\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cbb1d57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cbb1d57",
    "outputId": "69aa0e1f-6918-44e4-ce3b-e96129129774"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/train-clean-100/mfcc/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k2/4sq9hj556ds5m5fz707gl89r0000gn/T/ipykernel_11440/2970245272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     }\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/k2/4sq9hj556ds5m5fz707gl89r0000gn/T/ipykernel_11440/2970245272.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# If you want to use full Dataset, please pass None to csvpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtrain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibriSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LIBRI_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train-clean-100\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsvpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/train_filenames_subset_8192_v2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdev_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibriSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LIBRI_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev-clean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/k2/4sq9hj556ds5m5fz707gl89r0000gn/T/ipykernel_11440/1251875005.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, sample, shuffle, partition, csvpath)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpartition\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"/transcript/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train-clean-100/mfcc/'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(args, model, device, train_samples, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for i in range(len(train_samples)):\n",
    "        X, Y = train_samples[i]\n",
    "        train_items = LibriItems(X, Y, context=args['context'])\n",
    "        train_loader = torch.utils.data.DataLoader(train_items, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.float().to(device)\n",
    "            target = target.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % args['log_interval'] == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, dev_samples):\n",
    "    model.eval()\n",
    "    true_y_list = []\n",
    "    pred_y_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dev_samples)):\n",
    "            X, Y = dev_samples[i]\n",
    "\n",
    "            test_items = LibriItems(X, Y, context=args['context'])\n",
    "            test_loader = torch.utils.data.DataLoader(test_items, batch_size=args['batch_size'], shuffle=False)\n",
    "\n",
    "            for data, true_y in test_loader:\n",
    "                data = data.float().to(device)\n",
    "                true_y = true_y.long().to(device)                \n",
    "                \n",
    "                output = model(data)\n",
    "                pred_y = torch.argmax(output, axis=1)\n",
    "\n",
    "                pred_y_list.extend(pred_y.tolist())\n",
    "                true_y_list.extend(true_y.tolist())\n",
    "\n",
    "    train_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
    "    return train_accuracy\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = Network().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # If you want to use full Dataset, please pass None to csvpath\n",
    "    train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=\"/content/train_filenames_subset_8192_v2.csv\")\n",
    "    dev_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"dev-clean\")\n",
    "\n",
    "    for epoch in range(1, args['epoch'] + 1):\n",
    "        train(args, model, device, train_samples, optimizer, criterion, epoch)\n",
    "        test_acc = test(args, model, device, dev_samples)\n",
    "        print('Dev accuracy ', test_acc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = {\n",
    "        'batch_size': 2048,\n",
    "        'context': 0,\n",
    "        'log_interval': 200,\n",
    "        'LIBRI_PATH': '/content',\n",
    "        'lr': 0.001,\n",
    "        'epoch': 3\n",
    "    }\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw1p2_s22_starter_notebook.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "239d5711fd35d2bb9bad2d5ca41e22b79107b4494fe73df9033b517b748af271"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
